# Lost in the Middle: How Language Models Use Long Contexts (Experimental Approach)

**Paper:** Lost in the Middle: How Language Models Use Long Contexts (arXiv:2307.03172)

**Authors:** Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, Percy Liang

**Institutions:** Stanford University, UC Berkeley, Samaya AI

**ArXiv:** https://arxiv.org/abs/2307.03172 


**Presented by:** Nikki La, Nov 18

# Overview

**The Context**: Recent years have seen a race toward longer context windows - GPT-3.5 (4K), Claude (100K), even Gemini (1M+ tokens). The industry assumption has been: longer context = better performance.

**The Problem**: Do language models actually *use* their long context windows effectively? Can they access information regardless of where it appears?

**The Approach**: The researchers conducted controlled experiments using:
1. **Multi-document QA**: 20 Wikipedia documents, exactly one contains the answer, systematically vary which position contains the answer
2. **Key-value retrieval**: Pure lookup task with random UUIDs to test positional retrieval

**Key Innovation**: Instead of testing just "beginning vs. middle vs. end," they tested *every* position (1st, 5th, 10th, 15th, 20th) to reveal fine-grained patterns.

**How They Addressed It**: 
- Controlled variables: Same content, same question, only position changes
- Tested across 8 different models (GPT-3.5, GPT-4, Claude, etc.)
- Compared standard vs. extended context versions
- Tested both decoder-only and encoder-decoder architectures

**The Discovery**: All models show a U-shaped performance curve - performance drops 20%+ when information is in the middle. Even worse: giving GPT-3.5 the answer in the middle makes it perform *worse* than having no documents at all.

**The Implication**: Extended context models (16K, 100K tokens) show identical position curves to their base versions - they let you *fit* more content but don't help you *use* it better.


# Context and Problem

### The Context Window Arms Race

Recent improvements in hardware and algorithms have resulted in language models with increasingly large context windows (4K, 32K, even 100K tokens).

Models evaluated in this paper:
| **Model**              | **Context Length**      | **Source / Notes**                                       |
|--------------------------|-------------------------|-----------------------------------------------------------|
| MPT-30B-Instruct         | 8,192 tokens            | Section 2.2; uses ALiBi positional embeddings             |
| LongChat-13B (16K)       | 16,384 tokens           | Section 2.2; extends LLaMA-13B with condensed rotary embeddings |
| GPT-3.5-Turbo            | 4,096 tokens            | Section 2.2; OpenAI API                                   |
| GPT-3.5-Turbo-16K        | 16,384 tokens           | Section 2.2; OpenAI API                                   |
| Claude-1.3               | 8,192 tokens            | Section 2.2; Anthropic API                                |
| Claude-1.3-100K          | 100,000 tokens          | Section 2.2; Anthropic API                                |
| Flan-T5-XXL              | 512 tokens (training)   | Section 4.1; Raffel et al., 2020                          |
| Flan-UL2                 | 2,048 tokens (training) | Section 4.1; Tay et al., 2023                             |

### Real-World Applications Affected

"These input contexts can contain thousands of tokens, especially when language models are used to process long documents (e.g., legal or scientific documents, conversation histories, etc.) or when language models are augmented with external information (e.g., relevant documents from a search engine, database query results, etc.)"

**Applications that assume uniform context acces:**

- Retrieval-Augmented Generation (RAG)
- Document question answering
- Conversational AI with long histories
- Code analysis with large codebases
- Legal/scientific document analysis

### Research Question

"If language models can robustly use information within long input contexts, then their performance should be minimally affected by the position of the relevant information in the input context."

**This paper tests:** Does position of information in context affect model performance?

**The hypothesis:** If models truly "understand" their context, performance should be roughly the same regardless of where information appears.

# Approach

### Overview

This paper does **not** introduce a new model or algorithm. Instead, it provides a systematic **experimental framework** for evaluating position bias in existing language models.

**Core contribution**: A controlled evaluation methodology that reveals how model performance varies based on information position.

**Key innovation**: Systematic position variation (testing every 5th position) rather than random or coarse-grained testing (just beginning/middle/end).


### Task 1: Multi-Document Question Answering

**Purpose:** Test realistic retrieval scenario (mimics RAG applications)

**Data Source:** NaturalQuestions-Open (2,655 real Google search queries)

**Setup:**
Given:
- k documents (Wikipedia passages, each ‚â§100 tokens)
- Exactly 1 document contains the answer
- k-1 "distractor" documents (retrieved by Contriever - relevant but wrong)
- A question

Manipulation:
- Vary k ‚àà {10, 20, 30} documents
- Systematically place answer document at each position

Evaluation:
- Does model's output contain the correct answer?

<img width="743" height="302" alt="image" src="https://github.com/user-attachments/assets/c5c69ad0-7fb6-4675-99dd-da25ce906671" />

**Why distractors matter:** Using Contriever-retrieved distractors makes the task realistic - these are documents that seem relevant but don't contain the answer, just like real retrieval systems.


### Task 2: Key-Value Retrieval (Synthetic)

**Purpose:** Test minimal retrieval capability (no reasoning required)

**Setup:**
Given:
- JSON object with k key-value pairs
- All keys and values are random 128-bit UUIDs
- A target key to look up

Manipulation:
- Vary k ‚àà {75, 140, 300} pairs
- Systematically vary position of target pair

Evaluation:
- Does model return exact matching value?

<img width="739" height="299" alt="image" src="https://github.com/user-attachments/assets/e2030982-3dd5-4f16-b4f1-2bbc25c51aa7" />


**Why use UUIDs:** Removes all semantic/linguistic cues - tests pure positional retrieval ability.

**Controlled Variables**
- Context length: Number of documents/pairs
- Position: Where the relevant information appears
- Content: Everything else held constant

This allows isolating the effect of position on performance.

# Key Results

### Result 1: The U-Shaped Performance Curve

<img width="454" height="428" alt="image" src="https://github.com/user-attachments/assets/756fa1f7-31fb-4c37-bd6e-5a8e7729dad2" />

This is the paper's core finding. For GPT-3.5-Turbo with 20 documents:

**Quantitative data** (from Table 6, Appendix G.2):

| **Position**     | **Accuracy** | **Change from Position 1** | **vs. No Documents (56.1%)** | **Comparison** |
|------------------|--------------|-----------------------------|-------------------------------|----------------|
| üü¢ **1st (beginning)** | **75.8 %** | Baseline | + 19.7 % better | üü¢ Best |
| üü° **5th** | 57.2 % | ‚àí 18.6 % | + 1.1 % better | üü° Slightly better |
| üî¥ **10th (middle)** | 53.8 % | ‚àí 22.0 % | ‚àí 2.3 % worse | üî¥ Worse |
| üü† **15th** | 55.4 % | ‚àí 20.4 % | ‚àí 0.7 % worse | üü† Slightly worse |
| üü¢ **20th (end)** | 63.2 % | ‚àí 12.6 % | + 7.1 % better | üü¢ Better |

**The shocking finding** 
"When relevant information is placed in the middle of its input context, GPT-3.5-Turbo's performance on the multi-document question task is lower than its performance when predicting without any documents (i.e., the closed-book setting; 56.1%)."

**This means:** Giving the model the answer in the middle makes it perform worse than giving it no documents at all.

### Result 2: All Models Show This Pattern

Every model texted exhibits the U-shaped curve:
| **Model**              | **Position 1** | **Position 10** | **Position 20** | **Degradation** | **Trend** |
|-------------------------|----------------|------------------|------------------|------------------|------------|
|  **GPT-3.5-Turbo**         | 75.8 % | 53.8 % | 63.2 % | ‚àí22.0 % | üî¥ High drop |
|  **GPT-3.5-Turbo-16K**     | 75.7 % | 54.1 % | 63.1 % | ‚àí21.6 % | üî¥ High drop |
|  **Claude-1.3**            | 59.9 % | 56.8 % | 60.1 % | ‚àí3.1 %  | üü° Stable |
|  **Claude-1.3-100K**       | 59.8 % | 57.0 % | 60.0 % | ‚àí2.8 %  | üü¢ Very stable |
|  **MPT-30B-Instruct**      | 53.7 % | 52.2 % | 56.3 % | ‚àí1.5 %  | üü† Minimal drop |
|  **LongChat-13B-16K**      | 68.6 % | 55.3 % | 55.0 % | ‚àí13.3 % | üî¥ Noticeable drop |
|  **GPT-4**                 | 89 %   | 75 %   | 84 %   | ‚àí14.0 % | üü° Moderate drop |

**Critical observation** (page 5):

"We find that models often have identical performance to their extended-context counterparts, indicating that extended-context models are not necessarily better at using their input context."

**Extended-context models don't fix the problem:**
- GPT-3.5 vs. GPT-3.5-16K: Within 0.3% at every position
- Claude-1.3 vs. Claude-100K: Within 0.2% at every position

**Implication:** Extending context length lets you fit more information, but doesn't help you use it better.

### Result 3: It's Architectural, Not Fixable by Prompting

<img width="444" height="384" alt="image" src="https://github.com/user-attachments/assets/1447f4aa-e847-4194-8834-1ec0d52664a7" />

**The experiment:** Compare MPT-30B (base model, no instruction tuning) vs. MPT-30B-Instruct

"Even base language models (i.e., without instruction fine-tuning) show a U-shaped performance curve as we vary the position of relevant information in the input context."

Both models show nearly identical U-curves.

**What this means:** The position bias emerges during pre-training, not from instruction tuning.

**Why this happens:**

1. Pre-training creates recency bias:
    - Language models trained to predict next token: P(token_t | previous tokens)
    - In natural text, recent context is most predictive
    - Models learn to attend strongly to recent tokens

2. **Instruction tuning adds primacy bias:**
    - Instructions typically placed at beginning: [System prompt] [Data] [Question]
    - Models learn to attend to task setup at start
    - Nothing emphasizes middle content

3. **Result:** Strong beginning + strong end, weak middle (U-curve)


### Result 4: Query Duplication Only Helps Simple Retrieval

**The Experiment:** Place query before AND after documents
```
Standard:                   Query-Aware:
[Documents]                 [Query]  ‚Üê New: query at start
[Query]                     [Documents]
                            [Query]  ‚Üê Still at end
```
| **Task**              | **Standard** | **Query-Aware** | **Improvement** | **Notes** |
|------------------------|---------------|------------------|------------------|------------|
|  **Key-Value Retrieval** | 45 % | 100 % | +55 % | ‚úÖ Major gain |
|  **Multi-Doc QA**         | 54 % | 56 %  | +2 %  | ‚ö†Ô∏è Minimal improvement |

**Interpretation:** "Query-aware contextualization (placing the query before and after the documents or key-value pairs) enables near-perfect performance on the synthetic key-value task, but minimally changes trends in multi-document QA."

**Why:** Helps locate exact matches (key-value lookup), doesn't help reason about complex information.

### Result 5: More Documents Can Hurt

**The experiment:** 

**Results:**
| **# Documents** | **Retriever Recall** | **GPT-3.5 Accuracy** | **Claude Accuracy** | **Notes** |
|-----------------|----------------------|----------------------|---------------------|------------|
| 5  | 67 %  | 50 %  | 49 %  | üü† Moderate recall, lower accuracy |
| 10 | 75 %  | 53 %  | 52 %  | üü° Improved performance |
| 20 | 82 %  | 57 %  | 56 %  | üü¢ Noticeable accuracy gain |
| 30 | 86 %  | 58 %  | 57 %  | üü¢ Stable improvement |
| 50 | 90 %  | 58.5 % | 58 % | üü¢ Plateau reached |

**The critical gap:**
- Adding documents 20‚Üí50: Retriever improves +8%, Reader improves only +1.5%

"Using 50 documents instead of 20 retrieved documents only marginally improves performance (‚àº1.5% for GPT-3.5-Turbo and ‚àº1% for claude-1.3)."

**Why this matters:** More context can hurt because it creates more "middle" positions where the model struggles.



### Result 6: Encoder-Decoder Architecture Helps (Within Limits)

<img width="1114" height="336" alt="image" src="https://github.com/user-attachments/assets/52cc60b9-91e4-44e2-9ab3-78cf374fbb60" />

**The experiment:** Test Flan-UL2 (encoder-decoder) vs decoder-only models


**Flan-UL2 results:**

**Finding** (page 7):

"When Flan-UL2 is evaluated on sequences within its 2048-token training-time context window, its performance is relatively robust to changes in the position of relevant information within the input context (1.9% absolute difference between best- and worst-case performance)."

**But** (page 7):

"When evaluated on settings with sequences longer than 2048 tokens, Flan-UL2 performance begins to degrade when relevant information is placed in the middle."

## Question 1: Your Own RAG System

**Scenario**: You're building a chatbot that answers questions about your company's internal documentation. You plan to retrieve relevant documents and feed them to GPT-4.

### Question 1: Now that you know about the U-shaped performance curve, how would you design your system differently? What's one specific change you'd make?

<details>
<summary><b>Click to reveal answer</b></summary>

**Answer**: Use chunking - split retrieval into multiple focused API calls with ‚â§10 documents each, ensuring critical information stays at the edges (beginning or end) of each context window.

**Alternative**: Strategically rerank documents to place the most relevant ones at positions 1-3 (beginning) and the last 3 positions (end), accepting that middle documents will be weakly attended to.

</details>

## Question 2: The Extended Context Paradox

**Scenario**: Claude just announced a 1 million token context window. Your teammate is excited: "Now we can feed it entire codebases without chunking!"

### Question 2: Based on this paper's findings about extended context models (GPT-3.5 vs 16K, Claude vs 100K), what would you say to your teammate? Is their excitement justified?

<details>
<summary><b>Click to reveal answer</b></summary>

**Answer**: Extended context lets you *fit* more information, but doesn't help the model *use* it better - the position bias remains.

**Evidence from paper**: 
- GPT-3.5 vs GPT-3.5-16K: Identical performance curves (within 0.3%)
- Claude-1.3 vs Claude-100K: Identical performance curves (within 0.2%)

**Response to teammate**: "That's great for fitting large documents, but the model will still struggle with information in the middle. With 1M tokens, positions 100K-900K will likely have severe degradation (20%+ drop). We should still chunk strategically rather than dumping entire codebases in one context."

**The paradox**: More context capacity ‚â† better context usage.

</details>


## Why Encoder‚ÄìDecoder Helps

**Bidirectional Encoder**
- All tokens attend to all other tokens  
- Can process information in the context of the entire sequence  
- Position affects *relative weighting*, not absolute visibility  

**Critical Limitation**
- This advantage only holds **within the training sequence length**


### Model Architecture and Pseudocode
### Conceptual Architecture: How Position Bias Emerges

**Decoder-Only Models (GPT, Claude):**

```
Architecture:
  For each token position t:
    - Can only attend to positions 1 through t (causal masking)
    - Attention weights learned during training
    
  Attention pattern learned from pre-training:
    - Recent tokens (t-1, t-2, ...): HIGH attention
      ‚Üí Strong signal for next-token prediction
    - Distant tokens (t-100, t-200, ...): LOW attention
      ‚Üí Weak signal for next-token prediction
    
  Modified by instruction tuning:
    - First few tokens: MEDIUM-HIGH attention
      ‚Üí Contains task instructions
    - Last few tokens: HIGH attention
      ‚Üí Contains user query (recency)
    - Middle tokens: LOW attention
      ‚Üí No special emphasis
      
  Result: U-shaped attention distribution
```


**Encoder-Decoder Models (T5, Flan-UL2):**

```
Architecture:
  Encoder:
    - All tokens attend to all tokens (bidirectional)
    - Position represented via relative position embeddings
    - Can process token i in context of tokens i-k and i+k
    
  Decoder:
    - Attends to all encoder states
    - Can access any encoded information uniformly
    
  Result: More uniform attention distribution
  
  Limitation:
    - Positional embeddings learned up to training length
    - Beyond training length: extrapolation fails
    - U-curve emerges when extrapolating
```

**Pseudocode: Position-Controlled Evaluation**
```
ALGORITHM: EvaluatePositionBias
INPUT: 
  - dataset: List of (question, answer_doc, distractor_docs)
  - model: LanguageModel
  - num_documents: int (e.g., 20)
  - positions_to_test: List[int] (e.g., [1, 5, 10, 15, 20])

OUTPUT:
  - results: Dict[position -> accuracy]

PROCEDURE:
  results = {}
  
  FOR each position p IN positions_to_test:
    correct_count = 0
    total_count = 0
    
    FOR each (question, answer_doc, distractors) IN dataset:
      # Step 1: Construct document list with answer at position p
      documents = []
      distractor_index = 0
      
      FOR i FROM 1 TO num_documents:
        IF i == position:
          documents.append(answer_doc)
        ELSE:
          documents.append(distractors[distractor_index])
          distractor_index += 1
      
      # Step 2: Format prompt
      prompt = "Write a high-quality answer using only the provided search results.\n\n"
      FOR i, doc IN enumerate(documents):
        prompt += f"Document [{i+1}]: {doc}\n\n"
      prompt += f"Question: {question}\nAnswer:"
      
      # Step 3: Generate response
      response = model.generate(prompt)
      
      # Step 4: Check if correct
      ground_truth_answers = get_answers(question)
      is_correct = any(answer.lower() in response.lower() 
                      for answer in ground_truth_answers)
      
      IF is_correct:
        correct_count += 1
      total_count += 1
    
    # Step 5: Compute accuracy for this position
    results[position] = correct_count / total_count
  
  RETURN results
```


## Key Algorithm: Strategic Document Positioning (Novel Contribution)

**Previous work** (e.g., Ivgi et al., 2023): Only tested beginning vs. random positions

**This paper's innovation:** Fine-grained systematic testing of every position

```
ALGORITHM: SystematicPositionTest
INPUT:
  - num_documents: int (e.g., 20)
  - granularity: int (e.g., 5 ‚Üí test positions 1, 5, 10, 15, 20)

OUTPUT:
  - positions_to_test: List[int]

PROCEDURE:
  positions = []
  current_position = 1
  
  WHILE current_position <= num_documents:
    positions.append(current_position)
    current_position += granularity
  
  # Always include last position
  IF positions[-1] != num_documents:
    positions.append(num_documents)
  
  RETURN positions

EXAMPLE:
  SystematicPositionTest(20, 5) ‚Üí [1, 5, 10, 15, 20]
  
  Creates 5 test conditions:
    Condition 1: Answer at position 1  (beginning)
    Condition 2: Answer at position 5  (early)
    Condition 3: Answer at position 10 (middle)
    Condition 4: Answer at position 15 (late)
    Condition 5: Answer at position 20 (end)
```
**Why this matters:** Reveals U-shaped curve instead of just "beginning vs. middle vs. end"

**Comparison to Previous Approach**
| **Aspect**              | **Previous Work**                          | **This Paper**                                           |
|--------------------------|--------------------------------------------|----------------------------------------------------------|
| **Position Granularity** | Coarse (beginning / middle / end)          | Fine-grained (every 5th position)                        |
| **Control**              | Random positioning                         | Systematic controlled positioning                        |
| **Task Realism**         | Synthetic or next-word prediction          | Realistic multi-document QA (mimics RAG)                 |
| **Model Coverage**       | Mostly encoder‚Äìdecoder                     | Decoder-only **+** encoder‚Äìdecoder                       |
| **Extended Context**     | Not tested                                 | Direct comparison (e.g., 4K vs 16K vs 100K)              |

## Critical Analysis

### What Was Overlooked

**1. No Direct Attention Visualization**

**Issue**: The paper shows *performance* effects but doesn't directly measure *attention* patterns through attention weight analysis or heatmaps.

**Why it matters**: We infer that attention follows the U-curve based on performance, but don't have direct proof. Attention visualization would:
- Confirm the mechanism behind position bias
- Show which layers/heads contribute most to the bias
- Reveal if some heads attend uniformly while others show position bias

**Missing analysis**: Layer-by-layer attention patterns, head-specific attention distributions.

**2. Limited Analysis of Content Similarity**

**Issue**: All distractors are highly relevant (retrieved by Contriever with high similarity scores).

**Unexplored scenario**: What if distractors were random or low-relevance?
- Would position bias be less severe with clearly irrelevant distractors?
- Are models confused by similar-but-wrong content specifically?
- Does the difficulty of distinguishing answer from distractors amplify position effects?

**The paper mentions** (Appendix B) they tried random distractors but doesn't report detailed results or analysis.

**3. No Training Interventions**

**Issue**: The paper identifies the problem but doesn't test whether models could be *trained* to overcome position bias.

**Unexplored approaches**:
- Position-aware training: Explicitly vary answer position during training
- Adversarial training: Train on examples where answers are in middle positions
- Attention regularization: Penalize over-attending to edges during training
- Curriculum learning: Gradually increase context length with position variation

**Why it matters**: Without testing interventions, we don't know if position bias is fundamental or correctable.

**4. Limited Instruction Format Testing**

**Issue**: Only tested one prompt format ("Write a high-quality answer using only the provided search results").

**Unexplored variations**:
- Explicit position instructions: "Pay special attention to document 10"
- Numbered references with importance scores
- Hierarchical formatting with visual salience cues
- Different task framings

**Why it matters**: Different prompt engineering might mitigate (though probably not eliminate) position bias.

### What Could Be Developed Further

**1. Practical Remediation Strategies**

**What's missing**: The paper mentions chunking but provides limited guidance.

**Would be valuable**:
- Optimal chunk size experiments (they test 10/20/30, but what about 5/15/25?)
- Overlap window recommendations for chunking
- Cost-benefit analysis: chunking overhead vs. accuracy gains
- Comparison of aggregation methods when merging chunked results

**2. Domain-Specific Testing**

**Limitation**: Only tested on general knowledge QA (NaturalQuestions).

**Unexplored domains**:
- **Code**: Does syntax structure help models maintain attention? Can models track variable definitions across long code?
- **Structured data**: Do tables, JSON, or databases show different position effects?
- **Multi-lingual**: Do different languages show different bias patterns?
- **Specialized domains**: Medical texts, legal documents, scientific papers with domain-specific structure

**3. Interaction Effects**

**Missing analysis**:
- How does position bias interact with other known biases (e.g., recency in conversational contexts)?
- Does position bias worsen with increased distractor similarity?
- How do different types of questions (factual vs. reasoning vs. multi-hop) interact with position?

**4. Temporal Analysis**

**Unexplored**: Do newer model generations show reduced position bias?
- The paper tested GPT-3.5 and GPT-4
- Have GPT-4 Turbo, Claude 3, or Gemini 1.5 improved?
- Is position bias reducing over time with better training methods?

### Limitations

**1. Statistical Significance**

**Issue**: No confidence intervals, p-values, or significance tests reported.

**What we don't know**:
- Is 0.3% difference between GPT-3.5 and GPT-3.5-16K statistically significant?
- Are small variations across positions meaningful or noise?
- How many examples needed to reliably detect position effects?

**Impact**: Hard to know which findings are robust vs. which might be sampling artifacts.

**2. Token Count Variations**

**Issue**: Appendix F shows same documents = different token counts across models.
- Example: Same 10 documents = 1750 tokens (LongChat) vs. 1476 tokens (GPT-3.5)

**Why it matters**: 
- "Within training length" analysis becomes less clean
- Direct cross-model comparisons complicated
- Makes it harder to isolate pure position effects from token budget effects

**3. Limited Model Diversity**

**Missing architectures**:
- No sparse attention models (e.g., Longformer, BigBird)
- No retrieval-augmented models (e.g., RETRO)
- No models with learned position interpolation

**Why it matters**: These architectures might show different position bias patterns.

### Follow-Up Work

**Since publication** (2023), researchers have investigated whether newer models have addressed this:

**Follow-up findings**:
1. **Kuratov et al. (2024)**: "In Simple Attention Needed"
   - Position bias persists in GPT-4 Turbo and Claude 3
   - Newer models show *reduced* but not *eliminated* position effects

2. **Anthropic (2024)**: Claude 3 technical report
   - Explicitly addresses position bias in training
   - Reports improved middle-position performance
   - Still shows measurable degradation (5-10% vs. 20%+)

3. **Google (2024)**: Gemini 1.5 Pro paper
   - Tests on 1M token contexts
   - Position bias still present at extreme scales
   - Worse degradation at very long contexts (500K+)

4. **Liu et al. (2024)**: "Lost in the Middle" follow-up
   - Extended analysis to newer models
   - Confirms architectural nature of problem
   - Proposes attention mechanism modifications

**Consensus**: Problem is real, persistent, and architectural. Simple scaling doesn't solve it, but targeted architectural changes can reduce severity.

### Errors or Disputes

**No major errors identified** in the experimental design or analysis.

**No significant disputes** in the research community - findings have been replicated and extended.

**Minor notes**:
- Some researchers argue the "shocking" claim (middle worse than no docs) is overstated, as it only occurs at specific middle positions, not all middle content
- The closed-book baseline (56.1%) has high variance, making comparisons less clear

---

## Impact

### How This Changed AI Development

**Before this paper (pre-July 2023)**:
- Context length was the primary metric for model capability
- Industry assumed: longer context = strictly better performance
- Models evaluated on average/aggregate performance
- "100K context" used as marketing advantage
- RAG systems designed to maximize retrieved documents

**After this paper (post-July 2023)**:
- Context *usage quality* now evaluated alongside context length
- Position-based testing became standard for long-context model evaluation
- Model cards report best-case AND worst-case performance by position
- "Needle in haystack" tests became standard benchmarks
- RAG systems redesigned with position awareness

**Quote from paper** (page 11) - now widely cited in model evaluations:
> "To claim that a language model can robustly use information within long input contexts, it is necessary to show that its performance is minimally affected by the position of the relevant information in the input context."

### Real-World System Changes

**RAG System Frameworks**:

**LangChain** (2023-2024 updates):
- Added `ContextualCompressionRetriever` with position-aware ranking
- Implemented chunk-and-merge strategies for long documents
- Default retriever limits: 10-15 documents (previously 30-50)
- Documentation now warns about position bias

**LlamaIndex** (2023-2024 updates):
- Implemented `TreeSummarize` for hierarchical context processing
- Added position-aware reranking in retrievers
- Built-in chunking with strategic overlap
- "Lost in the Middle"-aware query engines

**Haystack** (Deepset):
- Position-aware document ranking algorithms
- Chunking strategies optimized for position bias
- Evaluation metrics that measure position effects

**Industry Guidelines**:

**Google Vertex AI**:
- Documentation recommends ‚â§20 documents for RAG
- Suggests placing most relevant documents at edges
- Provides position bias evaluation tools

**Microsoft Azure OpenAI**:
- Official docs warn about position bias (citing this paper)
- Recommends chunking for documents >10 pages
- Suggests query-aware reranking strategies

**AWS Bedrock**:
- Built-in chunking strategies that limit context size
- Position-aware retrieval modes
- Evaluation tools for position bias testing

**OpenAI**:
- GPT-4 Turbo documentation mentions position considerations
- Recommendations for structuring long prompts
- Chunking guidance for code analysis tasks

### Theoretical Importance

**Challenged fundamental assumptions**:

**1. Attention mechanism universality**:
- **Previous belief**: Self-attention is position-invariant
- **Reality**: Position strongly affects attention in practice
- **Implication**: Need new attention mechanisms or training procedures

**2. Context scaling laws**:
- **Previous belief**: Longer context improves performance linearly
- **Reality**: Non-linear, position-dependent effects
- **Implication**: Context length alone insufficient metric

**3. Transformer capabilities**:
- **Previous belief**: Transformers can access any context uniformly
- **Reality**: Strong architectural limitations even in SOTA models
- **Implication**: Need architectural innovations, not just scaling

**Opened research questions**:
1. Is position bias fundamental to autoregressive generation?
2. Can alternative attention mechanisms eliminate it?
3. What's the theoretical minimum for position variance?
4. Are there trade-offs between position invariance and other capabilities (e.g., generation quality)?

### Impact on Model Development

**Influenced training procedures**:
- Position-aware data augmentation during training
- Balanced position sampling in instruction tuning
- Attention regularization techniques

**Influenced architecture design**:
- Research into position-invariant attention
- Hybrid encoder-decoder approaches
- Sparse attention patterns that maintain middle attention

**Influenced evaluation standards**:
- Position bias now standard evaluation metric
- "Needle in haystack" tests in model cards
- Reporting worst-case alongside average performance

### Intersection With Other Work

**Built on**:

1. **Khandelwal et al. (2018)**: "Sharp Nearby, Fuzzy Far Away"
   - Found recency bias in LSTMs
   - This paper extends to Transformers and discovers primacy bias

2. **Sun et al. (2021)**: "Do Long-Range LMs Actually Use Long-Range Context?"
   - Found minimal long-range benefit for language modeling
   - This paper shows explicit retrieval tasks also suffer

3. **Ivgi et al. (2023)**: "Lost in the Middle" precursor (concurrent work)
   - Tested encoder-decoder models with coarse positioning
   - This paper adds fine-grained testing and decoder-only models

**Concurrent with**:

1. **Press et al. (2022)**: "Train Short, Test Long" (ALiBi)
   - Claimed ALiBi enables better length extrapolation
   - This paper shows it doesn't fix position bias (MPT-30B still shows U-curve)

2. **Dao et al. (2022)**: "FlashAttention"
   - Made long contexts computationally feasible
   - This paper shows: can *process* long context ‚â† can *use* it effectively

**Enabled future work**:

1. **Architecture modifications**:
   - Position-invariant attention mechanisms (e.g., Landmark Attention, 2024)
   - Hybrid approaches combining encoder-decoder for long context
   - Learned position interpolation methods

2. **Training innovations**:
   - Position-aware curriculum learning
   - Adversarial training for position robustness
   - Attention supervision techniques

3. **System-level solutions**:
   - Intelligent chunking algorithms
   - Position-aware retrieval strategies
   - Multi-pass reasoning systems

4. **Evaluation standards**:
   - Standardized position bias benchmarks (e.g., "Needle in Haystack")
   - Context usage efficiency metrics
   - Worst-case performance reporting requirements

### Long-Term Significance

**Changed how we think about context**:
- Context as a *resource* with uneven utility
- Position as a critical design consideration
- System design must work *with* rather than *against* position bias

**Industry impact**:
- Influenced billion-dollar decisions on context window development
- Shifted focus from "longer context" to "better context usage"
- Changed evaluation standards across the industry

**Research impact**:
- 500+ citations in first year
- Sparked entire research subfield on context usage
- Standard reference for position bias in LLMs

**Quote summarizing impact**: From concurrent review (NeurIPS 2023):
> "This work fundamentally changed how we evaluate and deploy long-context language models. It revealed that the context window arms race was missing a critical dimension: not just how much context models can accept, but how well they can use it."

---

## Resources

### Official Links

1. **Paper (arXiv)**: [https://arxiv.org/abs/2307.03172](https://arxiv.org/abs/2307.03172)
2. **Official Code Repository**: [https://github.com/nelson-liu/lost-in-the-middle](https://github.com/nelson-liu/lost-in-the-middle)
3. **Project Website**: [https://nelsonliu.me/papers/lost-in-the-middle](https://nelsonliu.me/papers/lost-in-the-middle)
4. **TACL Publication**: [Transactions of the Association for Computational Linguistics, 2023, Volume 11, Pages 284-299](https://direct.mit.edu/tacl)
5. **Lead Author Homepage**: [Nelson F. Liu - Stanford NLP](https://nelson-liu.github.io/)

### Related Papers

**Foundational transformer work**:
- Vaswani et al. (2017). "Attention Is All You Need" - [Link](https://arxiv.org/abs/1706.03762)
- Dai et al. (2019). "Transformer-XL" - [Link](https://arxiv.org/abs/1901.02860)

**Context usage analysis**:
- Khandelwal et al. (2018). "Sharp Nearby, Fuzzy Far Away" - [Link](https://arxiv.org/abs/1805.04623)
- Sun et al. (2021). "Do Long-Range Language Models Actually Use Long-Range Context?" - [Link](https://arxiv.org/abs/2109.09115)
- O'Connor & Andreas (2021). "What Context Features Can Transformer Language Models Use?" - [Link](https://arxiv.org/abs/2106.08293)

**Retrieval-augmented generation**:
- Izacard & Grave (2021). "Leveraging Passage Retrieval with Generative Models" - [Link](https://arxiv.org/abs/2007.01282)
- Ram et al. (2023). "In-Context Retrieval-Augmented Language Models" - [Link](https://arxiv.org/abs/2302.00083)
- Shi et al. (2023). "REPLUG: Retrieval-Augmented Black-Box Language Models" - [Link](https://arxiv.org/abs/2301.12652)

**Positional encodings**:
- Press et al. (2022). "Train Short, Test Long: Attention with Linear Biases" (ALiBi) - [Link](https://arxiv.org/abs/2108.12409)
- Su et al. (2021). "RoFormer: Enhanced Transformer with Rotary Position Embedding" - [Link](https://arxiv.org/abs/2104.09864)

**Long-context models**:
- Dao et al. (2022). "FlashAttention: Fast and Memory-Efficient Exact Attention" - [Link](https://arxiv.org/abs/2205.14135)
- Beltagy et al. (2020). "Longformer: The Long-Document Transformer" - [Link](https://arxiv.org/abs/2004.05150)

### Learning Resources

**Interactive visualizations**:
- **Transformer Explainer**: [https://poloclub.github.io/transformer-explainer](https://poloclub.github.io/transformer-explainer) - Interactive visualization of transformer architecture
- **LLM Visualization**: [https://bbycroft.net/llm](https://bbycroft.net/llm) - 3D visualization of LLM inference
- **Attention Visualization**: [https://transformer-circuits.pub](https://transformer-circuits.pub) - Mechanistic interpretability of transformers

**Blog posts and explanations**:
- Jay Alammar's "The Illustrated Transformer": [http://jalammar.github.io/illustrated-transformer](http://jalammar.github.io/illustrated-transformer)
- Lilian Weng's "Attention Mechanisms": [https://lilianweng.github.io/posts/2018-06-24-attention](https://lilianweng.github.io/posts/2018-06-24-attention)

**Datasets**:
- **NaturalQuestions-Open**: [https://github.com/google-research/natural-questions](https://github.com/google-research/natural-questions)
- **Contriever** (retrieval model): [https://github.com/facebookresearch/contriever](https://github.com/facebookresearch/contriever)

---

## Citation

Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., & Liang, P. (2023). Lost in the middle: How language models use long contexts. *Transactions of the Association for Computational Linguistics*, *11*, 284-299. https://doi.org/10.1162/tacl_a_00624

